{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB learning \n",
    "## Purpose of this notebook\n",
    "- Familiarize interfaces and functionalities of chromadb\n",
    "## Steps \n",
    "1. Setup ChromaDB locally and establish connection with the server through code  : Done \n",
    "2. Explore key concepts of ChromaDB \n",
    "    - Concept of 'collections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id2', 'id4']], 'embeddings': None, 'documents': [['sweet oranges', 'Donald Trump is the new president']], 'uris': None, 'data': None, 'metadatas': [[None, None]], 'distances': [[1.7286731004714966, 1.8054571151733398]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# switch `create_collection` to `get_or_create_collection` to avoid creating a new collection every time\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "'''\n",
    "\n",
    "'''\n",
    "# switch `add` to `upsert` to avoid adding the same documents every time\n",
    "collection.upsert(\n",
    "    documents=[\n",
    "        \"red pineapple\",\n",
    "        \"sweet oranges\",\n",
    "        \"Francis Suarez decided to invest in x.com\",\n",
    "        \"Donald Trump is the new president\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\",\"id3\",\"id4\"]\n",
    ")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document about florida\"], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections \n",
    "- Similar to Table concept of relational database \n",
    "- Documents are stored in a collection\n",
    "### What are documents \n",
    "- Smallest unit of information which can be embedded into a vector database\n",
    "- Analogus to a row in RDB\n",
    "- Choice of splitting information into document involves various constraints and considerations\n",
    "    1. Context length of the embedding model and the pipeline which follows the retrieval :\n",
    "        - Long content size of document may lead to bad performance for small context models. \n",
    "        - Trade of is between size of document and the overall information context. Small document split results in missing the overall picture of the information \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of vector db collections \n",
    "We need seperate collections for Local search vector retrieval pipeline. Following are the potential collections \n",
    "1. Entity collection\n",
    "    - This contains all the entities present in the graph\n",
    "    - Entities should be grouped based on the generated communities\n",
    "        - That is each Document may contain multiple entities. Hence combining random entities to generate document based on context limit doesn't make sense. \n",
    "2. Relationship vector store \n",
    "    - Same rationale for entities applies here also\n",
    "    - Group relationships based on communities.\n",
    "    - Split relationships in the communities into documents of fixed token count \n",
    "3. Text unit/chunk Collection\n",
    "    - Directly store each text chunk into this store\n",
    "    - Each chunk/unit becomes a document\n",
    "4. Community retport vector store\n",
    "    - Each community report is a document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded graph\n",
      "[{'description': 'A library that provides pre-written code to simplify the development of applications, particularly in the context of communication protocols. ; A library containing source code that provides support for various communication protocols, including IEC 60870-5. ; A library that provides functions for target applications to interact with, including general calls and callback functions. ; The Source Code Library is a component that interfaces with the Target Application (TA) through well-documented modules or header files. It provides a three-sided interface, with two sides representing calls back into the TA from the SCL Interface, and the third side representing calls into the SCL from the TA. ; A software framework used for developing applications that interact with various devices and systems. ; A software library that provides functionalities for handling database operations, message processing, and other tasks. It supports both synchronous and asynchronous database accesses based on configuration settings. ; A software library used for developing applications that interact with various communication protocols. ; A collection of software components used to implement communication protocols in electric utility systems.', 'id': 'SOURCE CODE LIBRARY (SCL)', 'type': 'TECHNOLOGIES AND FRAMEWORKS'}, {'description': 'The list of communication protocols currently supported by Triangle MicroWorks, Inc. Source Code Libraries. ; The communication protocols that the SCL supports, allowing it to interface with different devices and systems.', 'id': 'SUPPORTED PROTOCOLS', 'type': 'CONCEPTS'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivekv/anaconda3/envs/research_asst/lib/python3.12/site-packages/networkx/readwrite/json_graph/node_link.py:287: FutureWarning: \n",
      "The default value will be changed to `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_graph(data, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_graph(data, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import networkx as nx\n",
    "import os\n",
    "import itertools\n",
    "import chromadb\n",
    "\n",
    "def print_first_n_items_simple(my_dict, n):\n",
    "    for key, value in itertools.islice(my_dict.items(), n):\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "graph = None\n",
    "data:dict = None\n",
    "# /home/vivekv/Documents/RD/TheProphet/Odysseus/retrieval_stratergies/experiments/tmw_dnp3_scl_um_graph.yml\n",
    "with open(f\"{current_dir}/tmw_dnp3_scl_um_graph.yml\",\"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    graph = nx.node_link_graph(data)\n",
    "\n",
    "if(graph):\n",
    "    print(\"successfully loaded graph\")\n",
    "else:\n",
    "    print(\"failed to load\")\n",
    "\n",
    "edges = data['links']\n",
    "nodes = data['nodes']\n",
    "print(nodes[:2])\n",
    "\n",
    "\n",
    "# Collections initialization \n",
    "chroma_client = chromadb.Client()\n",
    "collection_text_units = chroma_client.get_or_create_collection(name=\"GraphRAG_Text_Units\") # Raw text units\n",
    "collection_entities = chroma_client.get_or_create_collection(name=\"GraphRAG_Entities\") # All entities will be stored here\n",
    "collection_relationships = chroma_client.get_or_create_collection(name=\"GraphRAG_Relationships\") # Relationship storage \n",
    "collection_community_reports = chroma_client.get_or_create_collection(name=\"GraphRAG_Community_Reports\") # Community reports \n",
    "\n",
    "\n",
    "def entity_collections(collection_entities,entities_list:list):\n",
    "    '''\n",
    "    given a list of entities and collection, attach them to one or more documents and save to collection\n",
    "    '''\n",
    "    if not entities_list:\n",
    "        raise ValueError(f\"Missing entities list input\")\n",
    "    \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Token counter-Begin-------------#\n",
    "def _estimate_tokens_hf(text: str, model_name: str = \"Qwen/Qwen2.5-Coder-14B\") -> int:\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens using Hugging Face tokenizers.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        model_name (str): The Hugging Face model name or path.\n",
    "    \n",
    "    Returns:\n",
    "        int: The estimated number of tokens.\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        return tokens.size(1)  # Token count\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading Hugging Face tokenizer for {model_name}: {e}\")\n",
    "    \n",
    "def _estimate_tokens_spm(text: str, model_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate tokens using SentencePiece tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        model_path (str): Path to the SentencePiece model file (.model).\n",
    "\n",
    "    Returns:\n",
    "        int: Estimated token count.\n",
    "    \"\"\"\n",
    "    import sentencepiece as spm\n",
    "    try:\n",
    "        sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "        tokens = sp.encode(text, out_type=str)\n",
    "        return len(tokens)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading SentencePiece model from {model_path}: {e}\")\n",
    "    \n",
    "def _estimate_tokens_tiktoken(text: str, model: str = \"gpt-4\",fallback: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens in a text string using the tiktoken library.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to estimate token count for.\n",
    "        model (str): The model name to determine the tokenizer (default: \"gpt-4\").\n",
    "    \n",
    "    Returns:\n",
    "        int: The estimated number of tokens.\n",
    "    \"\"\"\n",
    "    import tiktoken\n",
    "    try:\n",
    "        tokenizer = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Use fallback model\n",
    "        tokenizer = tiktoken.get_encoding(fallback)\n",
    "    return len(tokenizer.encode(text))\n",
    "    \n",
    "    # Encode the text and count the tokens\n",
    "    token_count = len(tokenizer.encode(text))\n",
    "    return token_count\n",
    "\n",
    "def estimate_tokens(text: str, method: str = \"hf\", **kwargs) -> int:\n",
    "    \"\"\"\n",
    "    Estimates the number of tokens in a given text using a specified tokenization method.\n",
    "\n",
    "    This function acts as a wrapper for multiple token estimation methods and dynamically selects the appropriate method based on the `method` parameter. It logs the token estimation process, including the chosen method and the token count. If the token count is zero, a warning is logged.\n",
    "\n",
    "    Args:\n",
    "        text (str): \n",
    "            The input text for which the token count is to be estimated.\n",
    "        method (str, optional): \n",
    "            The tokenization method to use. Defaults to `\"hf\"`. Supported methods are:\n",
    "            - `\"hf\"`: Hugging Face tokenization.\n",
    "            - `\"spm\"`: SentencePiece tokenization.\n",
    "            - `\"tiktoken\"`: OpenAI's TikToken tokenization.\n",
    "        **kwargs: \n",
    "            Additional keyword arguments passed to the underlying tokenization method for customization.\n",
    "\n",
    "    Returns:\n",
    "        int: \n",
    "            The estimated number of tokens in the input text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: \n",
    "            If an unsupported tokenization method is specified.\n",
    "\n",
    "    Workflow:\n",
    "        1. Log the selected tokenization method.\n",
    "        2. Use the specified method to estimate the number of tokens:\n",
    "            - If `method` is `\"hf\"`, call `_estimate_tokens_hf`.\n",
    "            - If `method` is `\"spm\"`, call `estimate_tokens_spm`.\n",
    "            - If `method` is `\"tiktoken\"`, call `estimate_tokens_tiktoken`.\n",
    "        3. Log a warning if the estimated token count is zero.\n",
    "        4. Log the final token count and return it.\n",
    "\n",
    "    Sub-functions:\n",
    "        - `_estimate_tokens_hf(text: str, **kwargs) -> int`: Estimates tokens using Hugging Face tokenization.\n",
    "        - `estimate_tokens_spm(text: str, **kwargs) -> int`: Estimates tokens using SentencePiece tokenization.\n",
    "        - `estimate_tokens_tiktoken(text: str, **kwargs) -> int`: Estimates tokens using OpenAI's TikToken.\n",
    "\n",
    "    Example Usage:\n",
    "        ```python\n",
    "        text = \"This is a sample text for token estimation.\"\n",
    "        \n",
    "        # Using Hugging Face tokenization\n",
    "        token_count_hf = estimate_tokens(text, method=\"hf\")\n",
    "        \n",
    "        # Using SentencePiece tokenization\n",
    "        token_count_spm = estimate_tokens(text, method=\"spm\")\n",
    "        \n",
    "        # Using TikToken tokenization\n",
    "        token_count_tiktoken = estimate_tokens(text, method=\"tiktoken\")\n",
    "        ```\n",
    "\n",
    "    Example Output:\n",
    "        For the input text `\"This is a sample text for token estimation.\"`, the token count may vary based on the method:\n",
    "        - Hugging Face: 10 tokens\n",
    "        - SentencePiece: 9 tokens\n",
    "        - TikToken: 11 tokens\n",
    "\n",
    "    Logging:\n",
    "        - Logs the selected tokenization method at the beginning.\n",
    "        - Logs a warning if the token count is zero, along with the input text.\n",
    "        - Logs the estimated token count.\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes that the tokenization methods `_estimate_tokens_hf`, `estimate_tokens_spm`, and `estimate_tokens_tiktoken` are implemented and available in the codebase.\n",
    "        - The choice of tokenization method should align with the downstream processing system.\n",
    "\n",
    "    Limitations:\n",
    "        - The token count may differ across methods due to variations in tokenization algorithms.\n",
    "        - If the input text is empty or poorly formatted, the token count may be zero, triggering a warning.\n",
    "    \"\"\"\n",
    "    # logging.info(f\"Estimating tokens using {method} method.\")\n",
    "    if method == \"hf\":\n",
    "        token_count = _estimate_tokens_hf(text, **kwargs)\n",
    "    elif method == \"spm\":\n",
    "        token_count = _estimate_tokens_spm(text, **kwargs)\n",
    "    elif method == \"tiktoken\":\n",
    "        token_count = _estimate_tokens_tiktoken(text, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported tokenization method: {method}\")\n",
    "    # if(token_count==0):\n",
    "    #     logging.warning(f\"Zero token count for following chunk:\\n{text}\\n\")\n",
    "    # logging.info(f\"Token count: {token_count}\")\n",
    "    return token_count\n",
    "#---------------Token counter-End---------------#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **ðŸ”¹ Core Interfaces for ChromaDB Wrapper**\n",
    "| **Functionality** | **Purpose** |\n",
    "|------------------|------------|\n",
    "| **1ï¸âƒ£ Store Document** | Add a document (text + metadata + embedding) to ChromaDB. |\n",
    "| **2ï¸âƒ£ Retrieve Document by ID** | Fetch a document using its unique ID. |\n",
    "| **3ï¸âƒ£ Query Documents by Metadata** | Filter and retrieve documents based on metadata fields. |\n",
    "| **4ï¸âƒ£ Semantic Search (Vector Search)** | Retrieve similar documents using embeddings. |\n",
    "| **5ï¸âƒ£ Update Document** | Modify an existing documentâ€™s metadata or embedding. |\n",
    "| **6ï¸âƒ£ Delete Document** | Remove a document from ChromaDB. |\n",
    "| **7ï¸âƒ£ Collection Management** | Create, list, and delete collections in ChromaDB. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored document with ID: f13e284f-ea24-4c72-b995-107d24c67ac0\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "import numpy as np\n",
    "from typing import Optional, List\n",
    "\n",
    "def store_document(collection, text: str, metadata: dict, embedding: Optional[List[float]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Stores a document in ChromaDB.\n",
    "\n",
    "    Args:\n",
    "        collection: The ChromaDB collection.\n",
    "        text (str): The document content.\n",
    "        metadata (dict): Metadata associated with the document.\n",
    "        embedding (Optional[List[float]]): Precomputed embedding (optional).\n",
    "\n",
    "    Returns:\n",
    "        str: Unique document ID.\n",
    "    \"\"\"\n",
    "    # Generate a unique document ID\n",
    "    doc_id = metadata.get(\"id\", str(uuid.uuid4()))\n",
    "    \n",
    "    # Compute embedding if not provided\n",
    "    if embedding is None:\n",
    "        embedding = np.random.rand(1536).tolist()  # Placeholder: Replace with real embedding model\n",
    "    \n",
    "    # Add text to metadata for retrieval context\n",
    "    metadata[\"text\"] = text\n",
    "\n",
    "    # Store document in ChromaDB\n",
    "    collection.add(embeddings=[embedding], metadatas=[metadata], ids=[doc_id])\n",
    "    \n",
    "    return doc_id\n",
    "\n",
    "\n",
    "# Test code \n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_or_create_collection(\"knowledge_base\")\n",
    "\n",
    "text = \"Artificial intelligence is transforming the world.\"\n",
    "metadata = {\"source\": \"article\", \"title\": \"AI Revolution\"}\n",
    "\n",
    "doc_id = store_document(collection, text, metadata)\n",
    "print(f\"Stored document with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed design of localsearch vector store implementation is done in corresponding files. Scope of this notebook is experiments with chromadb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering results with where clause ([Source](https://cookbook.chromadb.dev/core/filters/#in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:420: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:420: SyntaxWarning: invalid escape sequence '\\['\n",
      "/tmp/ipykernel_4038038/2147318356.py:420: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"\"\"\n",
      "/home/vivekv/anaconda3/envs/research_asst/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/tmp/ipykernel_4038038/2147318356.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_communities = torch.load(community_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing communities in the collection: {'community_1', 'community_12', 'community_4', 'community_3', 'community_7', 'community_8', 'community_10', 'needs_node_summary', 'community_2', 'community_9', 'community_5', 'community_0', 'needs_community_summary', 'community_11', 'community_6'}\n",
      "Adding following doc_ids to collection: ['community_1', 'community_12', 'community_4', 'community_3', 'community_7', 'community_8', 'community_10', 'community_2', 'community_9', 'community_5', 'community_0', 'community_11', 'community_6']\n",
      "Following are the missing_doc_ids:['INPUT SCHEMA', 'START NODE', 'STATEGRAPH', 'OUTPUT SCHEMA', 'GRAPH', 'RUNNABLELAMBDA', 'END NODE', 'NODES', 'END', 'MAP-REDUCE DESIGN PATTERN', 'EDGES', 'COGNITIVE ARCHITECTURE', 'MESSAGE OBJECTS', 'ANYMESSAGE', 'STATE', 'MESSAGEGRAPH', 'SCHEMA', 'PYDANTIC MODEL', 'REDUCER FUNCTIONS', 'OPERATOR.ADD', 'ANNOTATED', 'CUSTOMER SUPPORT APPLICATION', 'DEFAULT REDUCER', 'GRAPH STATE', 'MESSAGESSTATE', 'TYPEDDICT', 'ADD_MESSAGES FUNCTION', 'MESSAGE PASSING', 'RECURSION LIMIT', 'RECURSION LIMIT KEY', 'GRAPHRECURSIONERROR', 'SUPER-STEPS', 'LANGGRAPH', 'PREGEL', 'NODE_1', 'INPUTSTATE', 'REDUCERS', 'NODE_2', 'OVERALLSTATE', 'PRIVATESTATE', 'NODE_3', 'OUTPUTSTATE', 'HUMAN-IN-THE-LOOP', 'SEND', 'SUPERSTEP', 'GET_USER_INFO', 'LOOKUP_USER_INFO', 'NODE', 'EDGE', 'MULTI-AGENT HANDOFFS', 'CONDITIONAL EDGES', 'ROUTING FUNCTION', 'TOOLNODE', 'RUNNABLECONFIG', 'GRAPH RENDERING', 'INTERRUPT()', 'PLAYER', 'START', 'TOOLMESSAGE', 'COMMAND', 'PERSISTENCE LAYER', 'CHECKPOINTERS', 'COMPILE METHOD', 'GRAPH MIGRATIONS', 'BREAKPOINTS', 'TRANSFORM STATE', 'COMPILED SUBGRAPH', 'DATA VALIDATION', 'MULTI-AGENT SYSTEMS', 'YAML', 'PARENT GRAPH', 'STATE SCHEMA', 'SUBGRAPHS', 'SHARED KEYS', 'JSON SCHEMA', 'SUBGRAPH NODE FUNCTION', 'AIMESSAGE', 'HUMANMESSAGE', 'CHATMODEL', 'MESSAGE', 'SERIALIZATION', 'LANGCHAIN MESSAGES', 'HABIT', 'SUSTAINABLE ALTERNATIVE', 'BASESTORE', 'BASECHECKPOINTSAVER', 'THREADS', 'CONFIGURABLE KEY', 'CONFIGURATION SCHEMA', 'CONFIGURATION', 'CONFIGSCHEMA', 'COMMAND OBJECT', 'JSON SERIALIZABLE VALUE', 'HUMAN-IN-THE-LOOP WORKFLOWS', 'INTERRUPT FUNCTION']\n",
      "Following are the missing_doc_ids:['INPUT SCHEMA', 'START NODE', 'STATEGRAPH', 'OUTPUT SCHEMA', 'GRAPH', 'RUNNABLELAMBDA', 'END NODE', 'NODES', 'END', 'MAP-REDUCE DESIGN PATTERN', 'EDGES', 'COGNITIVE ARCHITECTURE', 'MESSAGE OBJECTS', 'ANYMESSAGE', 'STATE', 'MESSAGEGRAPH', 'SCHEMA', 'PYDANTIC MODEL', 'REDUCER FUNCTIONS', 'OPERATOR.ADD', 'ANNOTATED', 'CUSTOMER SUPPORT APPLICATION', 'DEFAULT REDUCER', 'GRAPH STATE', 'MESSAGESSTATE', 'TYPEDDICT', 'ADD_MESSAGES FUNCTION', 'MESSAGE PASSING', 'RECURSION LIMIT', 'RECURSION LIMIT KEY', 'GRAPHRECURSIONERROR', 'SUPER-STEPS', 'LANGGRAPH', 'PREGEL', 'NODE_1', 'INPUTSTATE', 'REDUCERS', 'NODE_2', 'OVERALLSTATE', 'PRIVATESTATE', 'NODE_3', 'OUTPUTSTATE', 'HUMAN-IN-THE-LOOP', 'SEND', 'SUPERSTEP', 'GET_USER_INFO', 'LOOKUP_USER_INFO', 'NODE', 'EDGE', 'MULTI-AGENT HANDOFFS', 'CONDITIONAL EDGES', 'ROUTING FUNCTION', 'TOOLNODE', 'RUNNABLECONFIG', 'GRAPH RENDERING', 'INTERRUPT()', 'PLAYER', 'START', 'TOOLMESSAGE', 'COMMAND', 'PERSISTENCE LAYER', 'CHECKPOINTERS', 'COMPILE METHOD', 'GRAPH MIGRATIONS', 'BREAKPOINTS', 'TRANSFORM STATE', 'COMPILED SUBGRAPH', 'DATA VALIDATION', 'MULTI-AGENT SYSTEMS', 'YAML', 'PARENT GRAPH', 'STATE SCHEMA', 'SUBGRAPHS', 'SHARED KEYS', 'JSON SCHEMA', 'SUBGRAPH NODE FUNCTION', 'AIMESSAGE', 'HUMANMESSAGE', 'CHATMODEL', 'MESSAGE', 'SERIALIZATION', 'LANGCHAIN MESSAGES', 'HABIT', 'SUSTAINABLE ALTERNATIVE', 'BASESTORE', 'BASECHECKPOINTSAVER', 'THREADS', 'CONFIGURABLE KEY', 'CONFIGURATION SCHEMA', 'CONFIGURATION', 'CONFIGSCHEMA', 'COMMAND OBJECT', 'JSON SERIALIZABLE VALUE', 'HUMAN-IN-THE-LOOP WORKFLOWS', 'INTERRUPT FUNCTION']\n",
      "Missing text_unit ids:['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
      "Following are the identified relationships:\n",
      "{'ids': ['(STATE,LANGGRAPH)', '(STATE,STATEGRAPH)', '(STATE,MESSAGEGRAPH)', '(STATE,PERSISTENCE LAYER)', '(NODES,LANGGRAPH)', '(NODES,STATEGRAPH)', '(EDGES,LANGGRAPH)', '(EDGES,STATEGRAPH)', '(LANGGRAPH,MESSAGE PASSING)', '(LANGGRAPH,PREGEL)', '(LANGGRAPH,SUPER-STEPS)', '(LANGGRAPH,SEND)', '(LANGGRAPH,COMMAND)', '(LANGGRAPH,OVERALLSTATE)', '(STATEGRAPH,START)', '(STATEGRAPH,END)', '(STATEGRAPH,PRIVATESTATE)', '(STATEGRAPH,START NODE)', '(STATEGRAPH,END NODE)', '(STATEGRAPH,CONDITIONAL EDGES)', '(STATEGRAPH,MAP-REDUCE DESIGN PATTERN)', '(STATEGRAPH,COGNITIVE ARCHITECTURE)', '(STATEGRAPH,SUBGRAPHS)', '(STATEGRAPH,PARENT GRAPH)', '(BREAKPOINTS,PERSISTENCE LAYER)', '(COMMAND,TOOLNODE)', '(HUMAN-IN-THE-LOOP,TOOLNODE)'], 'embeddings': None, 'documents': ['LangGraph uses a shared data structure called `State` to represent the current snapshot of your application.', 'The `State` of a `StateGraph` is more complex than just a list of messages, making it suitable for most applications. ; The state is processed by nodes within a StateGraph.', 'The `State` of a `MessageGraph` is ONLY a list of messages, which makes it suitable for chatbots but not for most other applications.', 'The state of a graph is managed by the persistence layer, which saves and retrieves it during execution.', 'LangGraph models agent workflows using Python functions called `Nodes`, which encode the logic of agents.', 'Nodes are added to the graph using the add_node method of StateGraph. ; A StateGraph consists of nodes that process state and configuration.', 'LangGraph uses Python functions called `Edges` to determine which `Node` to execute next based on the current `State`.', 'Edges define how logic is routed and how the graph decides to stop, being a crucial part of StateGraph. | Edges connect nodes in a StateGraph, defining the flow of processing.', \"LangGraph's underlying graph algorithm uses message passing to define a general program, where nodes send messages along edges.\", \"LangGraph is inspired by Google's Pregel system for large-scale graph processing.\", 'LangGraph uses super-steps in its algorithm to process the graph iteratively.', 'LangGraph supports returning `Send` objects from conditional edges.', 'LangGraph provides a way to combine control flow and state updates using `Command` objects.', \"LangGraph supports returning 'Send' objects from conditional edges, which can be used to pass state information to downstream nodes.\", 'START is a part of the StateGraph and marks the beginning of the process.', 'END is a part of the StateGraph and marks the conclusion of the process.', 'PrivateState can be declared within the graph by nodes, even if it was not passed during StateGraph initialization, as long as its schema is defined.', 'The START Node represents the entry point where user input is sent to the graph.', 'The END Node represents a terminal node in the graph, denoting which edges have no actions after they are done.', 'The StateGraph supports conditional edges, which allow optional routing to one or more edges.', 'The map-reduce design pattern is an example where the exact edges are not known ahead of time, similar to how StateGraph can handle dynamic states.', 'A cognitive architecture can be implemented using a StateGraph to process information and configurations. ; The cognitive architecture is implemented using a StateGraph, which allows for the creation and management of complex graph structures with configurable nodes and edges.', 'StateGraph is the framework or tool used to define and manage both subgraphs and their parent graphs.', 'StateGraph is used to define and manage both subgraphs and parent graphs, indicating a direct interaction between them.', 'Breakpoints rely on the persistence layer to save and restore the state of the graph at specific points during execution. ; Breakpoints rely on the persistence layer to save the state of the graph after each step, enabling stepping through execution and human-in-the-loop workflows.', 'The `ToolNode` prebuilt component automatically handles tools returning `Command` objects.', 'ToolNode automatically handles tools returning Command objects and propagates them to the graph state, which is important for understanding the full scope of human-in-the-loop interactions. ; ToolNode interacts with human-in-the-loop workflows by automatically handling tools returning Command objects and propagating them to the graph state, which is crucial for resuming execution after user input collection.'], 'uris': None, 'data': None, 'metadatas': [{'source': 'STATE', 'target': 'LANGGRAPH', 'weight': 9.0}, {'source': 'STATE', 'target': 'STATEGRAPH', 'weight': 17.0}, {'source': 'STATE', 'target': 'MESSAGEGRAPH', 'weight': 7.5}, {'source': 'STATE', 'target': 'PERSISTENCE LAYER', 'weight': 8.0}, {'source': 'NODES', 'target': 'LANGGRAPH', 'weight': 9.0}, {'source': 'NODES', 'target': 'STATEGRAPH', 'weight': 18.0}, {'source': 'EDGES', 'target': 'LANGGRAPH', 'weight': 9.0}, {'source': 'EDGES', 'target': 'STATEGRAPH', 'weight': 16.0}, {'source': 'LANGGRAPH', 'target': 'MESSAGE PASSING', 'weight': 8.5}, {'source': 'LANGGRAPH', 'target': 'PREGEL', 'weight': 8.0}, {'source': 'LANGGRAPH', 'target': 'SUPER-STEPS', 'weight': 7.5}, {'source': 'LANGGRAPH', 'target': 'SEND', 'weight': 8.0}, {'source': 'LANGGRAPH', 'target': 'COMMAND', 'weight': 9.0}, {'source': 'LANGGRAPH', 'target': 'OVERALLSTATE', 'weight': 8.0}, {'source': 'STATEGRAPH', 'target': 'START', 'weight': 6.0}, {'source': 'STATEGRAPH', 'target': 'END', 'weight': 6.0}, {'source': 'STATEGRAPH', 'target': 'PRIVATESTATE', 'weight': 5.0}, {'source': 'STATEGRAPH', 'target': 'START NODE', 'weight': 7.0}, {'source': 'STATEGRAPH', 'target': 'END NODE', 'weight': 7.0}, {'source': 'STATEGRAPH', 'target': 'CONDITIONAL EDGES', 'weight': 9.0}, {'source': 'STATEGRAPH', 'target': 'MAP-REDUCE DESIGN PATTERN', 'weight': 7.5}, {'source': 'STATEGRAPH', 'target': 'COGNITIVE ARCHITECTURE', 'weight': 16.0}, {'source': 'STATEGRAPH', 'target': 'SUBGRAPHS', 'weight': 7.0}, {'source': 'STATEGRAPH', 'target': 'PARENT GRAPH', 'weight': 8.5}, {'source': 'BREAKPOINTS', 'target': 'PERSISTENCE LAYER', 'weight': 15.5}, {'source': 'COMMAND', 'target': 'TOOLNODE', 'weight': 9.0}, {'source': 'HUMAN-IN-THE-LOOP', 'target': 'TOOLNODE', 'weight': 17.0}], 'included': [<IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n",
      "lenth of retrievals:27\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "from typing import List,Dict, Any\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "class LocalSearchUtils():\n",
    "    def __init__(self,source,client):\n",
    "        self.client = client\n",
    "        self.source = source\n",
    "        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "                                                    model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.collections_dict = self.setup_vector_databases(client=self.client)\n",
    "        self.collections_dict = self.load_collections(source=self.source,collections_dict=self.collections_dict)\n",
    "\n",
    "    def setup_vector_databases(self,client):\n",
    "        text_units_collection = client.get_or_create_collection(name=\"text_units\",embedding_function=self.embedding_function)\n",
    "        entities_collection = client.get_or_create_collection(name=\"entities\",embedding_function=self.embedding_function)\n",
    "        entity_descriptions = client.get_or_create_collection(name=\"entity_descriptions\",embedding_function=self.embedding_function)\n",
    "        relationships_collection = client.get_or_create_collection(name=\"relationships\",embedding_function=self.embedding_function)\n",
    "        community_summaries_collection = client.get_or_create_collection(name=\"community_summaries\",embedding_function=self.embedding_function)\n",
    "        return {\"text_units\":text_units_collection,\"entities\":entities_collection,\"entity_descriptions\":entity_descriptions,\n",
    "                \"relationships\":relationships_collection,\"community_summaries\":community_summaries_collection}\n",
    "\n",
    "    def load_collections(self,source,collections_dict):\n",
    "        cwd = os.getcwd()\n",
    "        artifacts_path = os.path.join(cwd,\"../../../artifacts/\")\n",
    "        community_path = f\"{artifacts_path}graphRAG/community/{source}/{source}_communities.pt\"\n",
    "        graph_path = f\"{artifacts_path}graph_extraction/{source}/{source}_graph.yml\"\n",
    "        text_units_path = f\"{artifacts_path}graph_extraction/{source}/{source}_chunks.yml\"\n",
    "\n",
    "        all_communities = torch.load(community_path)\n",
    "        with(open(graph_path,\"r\")) as loc_file:\n",
    "            graph = yaml.safe_load(loc_file)\n",
    "\n",
    "        with(open(text_units_path,\"r\")) as loc_file:\n",
    "            text_units = yaml.safe_load(loc_file)\n",
    "        # entities and communities can be collected from all_communities \n",
    "        # text units should be collected from text_chunks file\n",
    "        # relationships should be collected from graph.yml file\n",
    "\n",
    "        entities = {}\n",
    "        for key,community in all_communities.items():\n",
    "            if(key in ['needs_node_summary','needs_community_summary']):\n",
    "                continue\n",
    "            # entities|=community['nodes']\n",
    "            for entity_name, entity_data in community['nodes'].items():\n",
    "                entity_data['community'] = key  # Embed the community name inside the entity data\n",
    "                entities[entity_name] = entity_data\n",
    "        relationships = graph['links']\n",
    "        \n",
    "        # Logic to check if the collections contain any values or not. \n",
    "\n",
    "        collections_dict['community_summaries'] = self.store_communities(collection=collections_dict['community_summaries'],communities=all_communities)\n",
    "        collections_dict['entities']= self.store_entities(collection=collections_dict['entities'],entities=entities)\n",
    "        collections_dict['entity_descriptions'] = self.store_entity_descriptions(collection=collections_dict['entity_descriptions'],entities=entities)\n",
    "        collections_dict['relationships'] = self.store_relationships(collection=collections_dict['relationships'],relationships=relationships)\n",
    "        collections_dict['text_units'] = self.store_text_units(collection=collections_dict['text_units'],text_unit_list=text_units)\n",
    "        return collections_dict\n",
    "\n",
    "    #---------------store-Begin-------------#\n",
    "    def store_text_units(self,collection, text_unit_list: List[str]):\n",
    "        \"\"\"\n",
    "        Stores text units as ChromaDB documents in the given collection.\n",
    "\n",
    "        Args:\n",
    "            collection: ChromaDB collection where text units will be stored.\n",
    "            text_unit_list (List[str]): List of extracted text units.\n",
    "\n",
    "        Returns:\n",
    "            The updated ChromaDB collection.\n",
    "        \"\"\"\n",
    "        # Handle empty input\n",
    "        if not text_unit_list:\n",
    "            return collection  # No change\n",
    "        existing_docs = collection.get(include=[\"documents\"])\n",
    "        existing_ids = existing_docs[\"ids\"] if existing_docs and \"ids\" in existing_docs else []\n",
    "        # Generate document IDs using the text unit index\n",
    "        doc_ids = [str(i) for i in range(len(text_unit_list))]\n",
    "        missing_ids = [item for item in doc_ids if item not in existing_ids]\n",
    "        print(f\"Missing text_unit ids:{missing_ids}\")\n",
    "        if(missing_ids):\n",
    "            # Store text units in ChromaDB\n",
    "            collection.upsert(\n",
    "                ids=doc_ids,\n",
    "                documents=text_unit_list\n",
    "            )\n",
    "        return collection\n",
    "\n",
    "    def store_entity_descriptions(self,collection, entities: Dict[str, Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        Stores entity nodes into a ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            collection: The ChromaDB collection where entities will be stored.\n",
    "            entities (Dict[str, Dict[str, str]]): A dictionary where:\n",
    "                - Each key is a node name (acting as the node ID).\n",
    "                - Each value is a dictionary containing:\n",
    "                    - \"description\" (str): A detailed description of the entity.\n",
    "                    - \"summary\" (str): A concise summary of the description.\n",
    "                    - \"type\" (str): The category/type of the entity.\n",
    "\n",
    "        Processing:\n",
    "        - Each entity is converted into a document format for storage.\n",
    "        - The node name is used as the unique document ID.\n",
    "        - The \"type\" field is attached as metadata.\n",
    "        - The stored document consists of the concatenated \"summary\" and \"description\".\n",
    "\n",
    "        Returns:\n",
    "            The updated ChromaDB collection.\n",
    "        \"\"\"\n",
    "\n",
    "        # Handle empty input\n",
    "        if not entities:\n",
    "            return collection  # No change\n",
    "        existing_docs = collection.get(include=[\"documents\"])\n",
    "        existing_ids = existing_docs[\"ids\"] if existing_docs and \"ids\" in existing_docs else []\n",
    "\n",
    "        # Prepare data for batch insertion\n",
    "        doc_ids = list(entities.keys())  # Node names as IDs\n",
    "        missing_doc_ids = [item for item in doc_ids if item not in existing_ids]\n",
    "        print(f\"Following are the missing_doc_ids:{missing_doc_ids}\")\n",
    "        documents = [f\"{entities[id]['summary']}\\n{entities[id]['description']}\" for id in missing_doc_ids]\n",
    "        metadata_list = [{\"type\": entities[id][\"type\"]} for id in missing_doc_ids]\n",
    "\n",
    "        if(missing_doc_ids):\n",
    "            # Store entities in ChromaDB\n",
    "            collection.upsert(\n",
    "                ids=doc_ids,\n",
    "                documents=documents,\n",
    "                metadatas=metadata_list\n",
    "            )\n",
    "        return collection\n",
    "\n",
    "    def store_entities(self,collection,entities):\n",
    "        # Handle empty input\n",
    "        if not entities:\n",
    "            return collection  # No change\n",
    "        existing_docs = collection.get(include=[\"documents\"])\n",
    "        existing_ids = existing_docs[\"ids\"] if existing_docs and \"ids\" in existing_docs else []\n",
    "\n",
    "        # Prepare data for batch insertion\n",
    "        doc_ids = list(entities.keys())  # Node names as IDs\n",
    "        missing_doc_ids = [item for item in doc_ids if item not in existing_ids]\n",
    "\n",
    "        print(f\"Following are the missing_doc_ids:{missing_doc_ids}\")\n",
    "\n",
    "        documents = [f\"{entity}\" for entity in entities.keys()]\n",
    "        metadata_list = [{\"type\": entities[id][\"type\"]} for id in missing_doc_ids]\n",
    "\n",
    "        if(missing_doc_ids):\n",
    "            # Store entities in ChromaDB\n",
    "            collection.upsert(\n",
    "                ids=doc_ids,\n",
    "                documents=documents,\n",
    "                metadatas=metadata_list\n",
    "            )\n",
    "        return collection\n",
    "\n",
    "    def store_relationships(self,collection, relationships: List[Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        Stores relationship data into a ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            collection: The ChromaDB collection where relationships will be stored.\n",
    "            relationships (List[Dict[str, str]]): A list of relationship dictionaries, \n",
    "                where each dictionary contains:\n",
    "                - \"description\" (str): A textual explanation of the relationship.\n",
    "                - \"source\" (str): The starting node of the relationship.\n",
    "                - \"target\" (str): The ending node of the relationship.\n",
    "                - \"weight\" (float): A numerical value representing the relationship strength.\n",
    "\n",
    "        Processing:\n",
    "        - First, check which relationship IDs are **already stored** in the collection.\n",
    "        - Filter out existing IDs and insert **only missing relationships**.\n",
    "        - The unique ID for each relationship is generated in the format: \"(<source>,<target>)\".\n",
    "        - The \"description\" field is stored as the document content.\n",
    "        - The metadata includes:\n",
    "            - \"source\" (str): The starting node.\n",
    "            - \"target\" (str): The ending node.\n",
    "            - \"weight\" (float): Strength of the relationship.\n",
    "\n",
    "        Returns:\n",
    "            The updated ChromaDB collection.\n",
    "        \"\"\"\n",
    "\n",
    "        # Handle empty input\n",
    "        if not relationships:\n",
    "            return collection  # No change\n",
    "\n",
    "        # Step 1: Get existing document IDs in the collection\n",
    "        existing_docs = collection.get(include=[\"documents\"])\n",
    "        existing_ids = set(existing_docs[\"ids\"]) if existing_docs and \"ids\" in existing_docs else set()\n",
    "\n",
    "        doc_ids = []\n",
    "        documents = []\n",
    "        metadata_list = []\n",
    "\n",
    "        # Step 2: Filter out already existing relationships\n",
    "        for rel in relationships:\n",
    "            if \"description\" in rel and \"source\" in rel and \"target\" in rel and \"weight\" in rel:\n",
    "                # Generate unique relationship ID\n",
    "                rel_id = f\"({rel['source']},{rel['target']})\"\n",
    "\n",
    "                if rel_id in existing_ids:\n",
    "                    continue  # Skip existing relationships\n",
    "\n",
    "                # Append only missing relationships\n",
    "                doc_ids.append(rel_id)\n",
    "                documents.append(rel[\"description\"])\n",
    "                metadata_list.append({\n",
    "                    \"source\": rel[\"source\"],\n",
    "                    \"target\": rel[\"target\"],\n",
    "                    \"weight\": rel[\"weight\"]\n",
    "                })\n",
    "\n",
    "        # Step 3: Store only new relationships in ChromaDB\n",
    "        if doc_ids:  # Only store if there are new relationships\n",
    "            collection.upsert(\n",
    "                ids=doc_ids,\n",
    "                documents=documents,\n",
    "                metadatas=metadata_list\n",
    "            )\n",
    "\n",
    "        return collection\n",
    "\n",
    "    def store_communities(self,collection, communities: Dict[str, Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        Stores community data into a ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            collection: The ChromaDB collection where communities will be stored.\n",
    "            communities (Dict[str, Dict[str, str]]): A dictionary containing various keys,\n",
    "                but only the following keys are of interest:\n",
    "                - \"title\" (str): The name or title of the community (used as metadata).\n",
    "                - \"summary\" (str): A brief description summarizing the community.\n",
    "                - \"metadata\" (dict): A dictionary containing:\n",
    "                    - \"community_id\" (int): A unique ID derived from the GraphRAG community.\n",
    "                    - \"node_count\" (int): Number of nodes in the community.\n",
    "                    - \"internal_edge_count\" (int): Number of internal edges.\n",
    "                    - \"external_edge_count\" (int): Number of external edges.\n",
    "                    - \"list_of_nodes\" (list(str)): List of member nodes\n",
    "        Processing:\n",
    "        - First, check which community IDs are **already stored** in the collection.\n",
    "        - Filter out existing IDs and insert **only missing communities**.\n",
    "        - The \"community_id\" is used as the **unique document ID**.\n",
    "        - The \"summary\" is stored as the document content.\n",
    "        - Metadata includes:\n",
    "            - \"title\" (str): The title of the community.\n",
    "            - \"node_count\" (int): Total number of nodes.\n",
    "            - \"total_edges\" (int): Sum of \"internal_edge_count\" and \"external_edge_count\".\n",
    "\n",
    "        Returns:\n",
    "            The updated ChromaDB collection.\n",
    "        \"\"\"\n",
    "\n",
    "        # Handle empty input\n",
    "        if not communities:\n",
    "            return collection  # No change\n",
    "\n",
    "        # Step 1: Get existing document IDs in the collection\n",
    "        existing_docs = collection.get(include=[\"documents\"])\n",
    "        existing_ids = existing_docs[\"ids\"] if existing_docs and \"ids\" in existing_docs else []\n",
    "        missing_ids = communities.keys() - existing_ids\n",
    "        print(f\"Missing communities in the collection: {missing_ids}\")\n",
    "\n",
    "        doc_ids = []\n",
    "        documents = []\n",
    "        metadata_list = []\n",
    "\n",
    "        # Step 2: Filter out already existing communities\n",
    "        for community_id in missing_ids:\n",
    "            data = communities[community_id]\n",
    "            if \"title\" in data and \"summary\" in data and \"metadata\" in data: # avoid other keys except communities from the dictionary \n",
    "                # Convert community_id to string to match stored document IDs\n",
    "                community_id_str = str(community_id)\n",
    "                title = data[\"title\"]\n",
    "                summary = data[\"summary\"]\n",
    "                metadata = data[\"metadata\"]\n",
    "                metadata[\"nodes\"] = [node for node in data[\"nodes\"].keys()] \n",
    "\n",
    "                # Extract node count and total edges\n",
    "                node_count = metadata.get(\"node_count\", 0)\n",
    "                total_edges = metadata.get(\"internal_edge_count\", 0) + metadata.get(\"external_edge_count\", 0)\n",
    "\n",
    "                # Append only missing communities\n",
    "                doc_ids.append(community_id_str)\n",
    "                documents.append(summary)\n",
    "                metadata_list.append({\"node_count\": node_count, \"title\": title, \"total_edges\": total_edges})\n",
    "\n",
    "        # Step 3: Store only new communities in ChromaDB\n",
    "        if doc_ids:  # Only store if there are new communities\n",
    "            print(f\"Adding following doc_ids to collection: {doc_ids}\")\n",
    "            collection.upsert(\n",
    "                ids=doc_ids,\n",
    "                documents=documents,\n",
    "                metadatas=metadata_list\n",
    "            )\n",
    "\n",
    "        return collection\n",
    "    #---------------store-End---------------#\n",
    "\n",
    "    #---------------search-Begin-------------#\n",
    "    def search_text_units(self,collection, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Given a query text, retrieve the top k most relevant text units from the collection.\n",
    "\n",
    "        Args:\n",
    "            collection: The ChromaDB collection for text units.\n",
    "            query (str): The search query.\n",
    "            k (int): The number of top results to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of top k matching text units.\n",
    "        \"\"\"\n",
    "        results = collection[\"text_units\"].query(query_texts=[query], n_results=k)\n",
    "        return results[\"documents\"][0] if results and \"documents\" in results else []\n",
    "\n",
    "    def search_entities(self, query: str,collection_dict: Dict[str, Any]= None, k: int = 3) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Searches for the most relevant entities based on a given query.\n",
    "\n",
    "        This function performs vector similarity search on both:\n",
    "        1. **Entity Names Collection** â€“ Finds entities directly by name.\n",
    "        2. **Entity Descriptions Collection** â€“ Finds entities based on their descriptions.\n",
    "\n",
    "        The results from both sources are merged using **harmonic mean** to prioritize entities that appear in both searches.\n",
    "\n",
    "        Args:\n",
    "            collection_dict (Dict[str, Any]): A dictionary of ChromaDB collections containing:\n",
    "                - \"entities\": ChromaDB collection for entity names.\n",
    "                - \"entity_descriptions\": ChromaDB collection for entity descriptions.\n",
    "            query (str): The search query used for entity retrieval.\n",
    "            k (int): The number of top results to retrieve from each search (default: 3).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, Dict[str, Any]]]: A dictionary containing:\n",
    "                - \"entity_name_store\": Entities retrieved based on names.\n",
    "                - \"entity_desc_store\": Entities retrieved based on descriptions.\n",
    "                Each entity is stored as:\n",
    "                ```json\n",
    "                {\n",
    "                    \"<entity_name>\": {\n",
    "                        \"type\": \"<entity_type>\",\n",
    "                        \"distance\": <retrieval_distance>,\n",
    "                        \"description\": \"<entity_description>\"\n",
    "                    }\n",
    "                }\n",
    "                ```\n",
    "\n",
    "        Example:\n",
    "            ```python\n",
    "            collection_dict = {\n",
    "                \"entities\": chromadb.PersistentClient().get_collection(\"entity_names\"),\n",
    "                \"entity_descriptions\": chromadb.PersistentClient().get_collection(\"entity_descriptions\"),\n",
    "            }\n",
    "            results = search_entities(collection_dict, \"AI research\", k=3)\n",
    "            print(results)\n",
    "            ```\n",
    "        \"\"\"\n",
    "\n",
    "        def reconstruct_entities(retrieved_docs: Dict[str, Any], distances: List[float] = []) -> Dict[str, Dict[str, Any]]:\n",
    "            \"\"\"\n",
    "            Reconstructs the entities dictionary from a ChromaDB retrieval response.\n",
    "\n",
    "            This function extracts entity names, types, descriptions, and retrieval distances\n",
    "            to form a structured dictionary.\n",
    "\n",
    "            Args:\n",
    "                retrieved_docs (Dict[str, Any]): The dictionary returned from a ChromaDB retrieval query.\n",
    "                distances (List[float], optional): A list of distances (lower means higher similarity).\n",
    "                    - If provided, it overrides the distances extracted from `retrieved_docs`.\n",
    "\n",
    "            Returns:\n",
    "                Dict[str, Dict[str, Any]]: A dictionary where each key is an entity name, and the value contains:\n",
    "                    - \"type\": The entity classification.\n",
    "                    - \"distance\": The similarity score (lower is better).\n",
    "                    - \"description\": A detailed explanation of the entity.\n",
    "\n",
    "            Example Output:\n",
    "                ```json\n",
    "                {\n",
    "                    \"TOOLNODE\": {\n",
    "                        \"type\": \"TECHNOLOGIES AND FRAMEWORKS\",\n",
    "                        \"distance\": 0.6034,\n",
    "                        \"description\": \"A prebuilt component in LangGraph that handles tools returning `Command` objects.\"\n",
    "                    }\n",
    "                }\n",
    "                ```\n",
    "            \"\"\"\n",
    "\n",
    "            entities = {}\n",
    "\n",
    "            if distances:\n",
    "                ids_list = retrieved_docs.get(\"ids\", [[]])  # List of entity names\n",
    "                documents_list = retrieved_docs.get(\"documents\", [[]])  # List of descriptions\n",
    "                metadatas_list = retrieved_docs.get(\"metadatas\", [[]])  # List of metadata dictionaries\n",
    "                distances_list = distances  # Use provided distances\n",
    "            else:\n",
    "                ids_list = retrieved_docs.get(\"ids\", [[]])[0]\n",
    "                documents_list = retrieved_docs.get(\"documents\", [[]])[0]\n",
    "                metadatas_list = retrieved_docs.get(\"metadatas\", [[]])[0]\n",
    "                distances_list = retrieved_docs.get(\"distances\", [[]])[0]\n",
    "\n",
    "            num_results = min(len(ids_list), len(documents_list), len(metadatas_list), len(distances_list))\n",
    "\n",
    "            for i in range(num_results):\n",
    "                entity_name = ids_list[i]\n",
    "                entity_type = metadatas_list[i].get(\"type\", \"UNKNOWN\") if metadatas_list else \"UNKNOWN\"\n",
    "                distance = distances_list[i]\n",
    "                description = documents_list[i]\n",
    "\n",
    "                entities[entity_name] = {\n",
    "                    \"type\": entity_type,\n",
    "                    \"distance\": distance,\n",
    "                    \"description\": description\n",
    "                }\n",
    "\n",
    "            return entities\n",
    "\n",
    "        def merge_retrieved_dicts(dict1: Dict[str, Dict[str, Any]], dict2: Dict[str, Dict[str, Any]]):\n",
    "            \"\"\"\n",
    "            Merges two retrieved entity dictionaries based on common keys.\n",
    "\n",
    "            If an entity appears in both dictionaries, their 'distance' values are updated using **harmonic mean**:\n",
    "                \\[\n",
    "                new\\_distance = \\frac{2}{(\\frac{1}{d1} + \\frac{1}{d2})}\n",
    "                \\]\n",
    "            This ensures that frequently appearing entities are given **higher priority** in the retrieval results.\n",
    "\n",
    "            Args:\n",
    "                dict1 (Dict[str, Dict[str, Any]]): The primary dictionary to update.\n",
    "                dict2 (Dict[str, Dict[str, Any]]): The secondary dictionary, from which matching elements are merged and removed.\n",
    "\n",
    "            Returns:\n",
    "                None: dict1 is updated in place, and matching keys are removed from dict2.\n",
    "\n",
    "            Example:\n",
    "                ```python\n",
    "                dict1 = {\"TOOLNODE\": {\"distance\": 0.60}}\n",
    "                dict2 = {\"TOOLNODE\": {\"distance\": 0.75}}\n",
    "                merge_retrieved_dicts(dict1, dict2)\n",
    "                print(dict1)  # TOOLNODE distance will be updated to ~0.667\n",
    "                ```\n",
    "            \"\"\"\n",
    "            common_keys = set(dict1.keys()) & set(dict2.keys())  # Find common entities\n",
    "\n",
    "            for key in common_keys:\n",
    "                d1 = dict1[key][\"distance\"]\n",
    "                d2 = dict2[key][\"distance\"]\n",
    "\n",
    "                # Compute harmonic mean of distances (lower is better)\n",
    "                new_distance = 2 / ((1 / d1) + (1 / d2))\n",
    "\n",
    "                dict1[key][\"distance\"] = new_distance  # Update dict1\n",
    "                del dict2[key]  # Remove from dict2\n",
    "        \n",
    "        if(not collection_dict):\n",
    "            collection_dict = self.collections_dict\n",
    "\n",
    "        # Step 1: Query both collections (entity names and descriptions)\n",
    "        entity_name_retrievals = collection_dict[\"entities\"].query(query_texts=[query], n_results=k)\n",
    "        entity_desc_retrievals = collection_dict[\"entity_descriptions\"].query(query_texts=[query], n_results=k)\n",
    "\n",
    "        # Step 2: Process retrievals\n",
    "        results = {\"entity_name_store\": {}, \"entity_desc_store\": {}}\n",
    "\n",
    "        if entity_name_retrievals:\n",
    "            entities_to_get = entity_name_retrievals.get(\"ids\", [[]])[0]\n",
    "            distances = entity_name_retrievals.get(\"distances\", [[]])[0]\n",
    "            name_retrievals_with_desc = collection_dict[\"entity_descriptions\"].get(ids=entities_to_get)\n",
    "            results[\"entity_name_store\"] = reconstruct_entities(name_retrievals_with_desc, distances=distances)\n",
    "\n",
    "        if entity_desc_retrievals:\n",
    "            results[\"entity_desc_store\"] = reconstruct_entities(entity_desc_retrievals)\n",
    "\n",
    "        # Step 3: Merge entity name and description retrievals\n",
    "        merge_retrieved_dicts(results[\"entity_name_store\"], results[\"entity_desc_store\"])\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_relationships(self,collection,query, **kwargs) -> List[str]:\n",
    "        \"\"\"\n",
    "        Given a query text, retrieve the top k most relevant relationships from the collection.\n",
    "\n",
    "        Args:\n",
    "            collection: The ChromaDB collection for relationships.\n",
    "            query (str): The search query.\n",
    "            k (int): The number of top results to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of top k matching relationships.\n",
    "        \"\"\"\n",
    "        if query:\n",
    "            results = collection[\"relationships\"].query(**kwargs)\n",
    "        else:\n",
    "            results = collection[\"relationships\"].get(**kwargs)\n",
    "        return results\n",
    "\n",
    "    def search_community_summaries(self,collection, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Given a query text, retrieve the top k most relevant community summaries from the collection.\n",
    "\n",
    "        Args:\n",
    "            collection: The ChromaDB collection for community summaries.\n",
    "            query (str): The search query.\n",
    "            k (int): The number of top results to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of top k matching community summaries.\n",
    "        \"\"\"\n",
    "        results = collection[\"relationships\"].query(query_texts=[query], n_results=k)\n",
    "        return results[\"documents\"][0] if results and \"documents\" in results else []\n",
    "\n",
    "\n",
    "import os \n",
    "cwd = os.getcwd()\n",
    "client = chromadb.PersistentClient(\n",
    "                path=f\"{cwd}../graphRAG_vector_db\",\n",
    "                settings=Settings(),\n",
    "                tenant=DEFAULT_TENANT,\n",
    "                database=DEFAULT_DATABASE,\n",
    "            )\n",
    "\n",
    "# source = \"tmw_sdg_um\"\n",
    "source = \"low_level\"\n",
    "loc_search_db = LocalSearchUtils(client=client,source=source)\n",
    "\n",
    "entities = [\"STATEGRAPH\",\"MESSAGEGRAPH\",\"LANGGRAPH\",\"PERSISTENCE LAYER\",\"TOOLNODE\"] # get from state\n",
    "#---------------Query to get relationships-Begin-------------#\n",
    "relationships = loc_search_db.search_relationships(collection=loc_search_db.collections_dict,query=False,\n",
    "    where={\"$or\":[{\"source\": {\"$in\": entities}},{\"target\": {\"$in\": entities}}]},\n",
    "    include=[\"documents\",\"metadatas\"]\n",
    "    )\n",
    "#---------------Query to get relationships-End---------------#\n",
    "#---------------Query to get all communities-Begin-------------#\n",
    "# communities = loc_search_db.search_communities(collection=loc_search_db.collections_dict,query=False,\n",
    "#     where={\"nodes\": {\"$in\": entities}},\n",
    "#     include=[\"documents\",\"metadatas\"]\n",
    "#     )\n",
    "#---------------Query to get all communities-End---------------#\n",
    "\n",
    "print(f\"Following are the identified relationships:\\n{relationships}\")\n",
    "print(f\"lenth of retrievals:{len(relationships['ids'])}\")\n",
    "# for reln in relationships['ids'][0]:\n",
    "#     print(f\"{reln}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "- If `n_results` is not specified, query return top 10 results by default.\n",
    "- `n_results` parameter is not available for get method\n",
    "- `get` method is used to retrieve blindly without vector search. `query` does vector search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_asst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
