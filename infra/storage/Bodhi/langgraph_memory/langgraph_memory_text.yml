"# Memory\n\n## What is Memory?\n\n[Memory](https://pmc.ncbi.nlm.nih.gov/articles/PMC10410470/)\
  \ is a cognitive function that allows people to store, retrieve, and use information\
  \ to understand their present and future. Consider the frustration of working with\
  \ a colleague who forgets everything you tell them, requiring constant repetition!\
  \ As AI agents undertake more complex tasks involving numerous user interactions,\
  \ equipping them with memory becomes equally crucial for efficiency and user satisfaction.\
  \ With memory, agents can learn from feedback and adapt to users' preferences. This\
  \ guide covers two types of memory based on recall scope:\n\n**Short-term memory**,\
  \ or [thread](persistence.md#threads)-scoped memory, can be recalled at any time\
  \ **from within** a single conversational thread with a user. LangGraph manages\
  \ short-term memory as a part of your agent's [state](low_level.md#state). State\
  \ is persisted to a database using a [checkpointer](persistence.md#checkpoints)\
  \ so the thread can be resumed at any time. Short-term memory updates when the graph\
  \ is invoked or a step is completed, and the State is read at the start of each\
  \ step.\n\n**Long-term memory** is shared **across** conversational threads. It\
  \ can be recalled _at any time_ and **in any thread**. Memories are scoped to any\
  \ custom namespace, not just within a single thread ID. LangGraph provides [stores](persistence.md#memory-store)\
  \ ([reference doc](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore))\
  \ to let you save and recall long-term memories.\n\nBoth are important to understand\
  \ and implement for your application.\n\n![](img/memory/short-vs-long.png)\n\n##\
  \ Short-term memory\n\nShort-term memory lets your application remember previous\
  \ interactions within a single [thread](persistence.md#threads) or conversation.\
  \ A [thread](persistence.md#threads) organizes multiple interactions in a session,\
  \ similar to the way email groups messages in a single conversation.\n\nLangGraph\
  \ manages short-term memory as part of the agent's state, persisted via thread-scoped\
  \ checkpoints. This state can normally include the conversation history along with\
  \ other stateful data, such as uploaded files, retrieved documents, or generated\
  \ artifacts. By storing these in the graph's state, the bot can access the full\
  \ context for a given conversation while maintaining separation between different\
  \ threads.\n\nSince conversation history is the most common form of representing\
  \ short-term memory, in the next section, we will cover techniques for managing\
  \ conversation history when the list of messages becomes **long**. If you want to\
  \ stick to the high-level concepts, continue on to the [long-term memory](#long-term-memory)\
  \ section.\n\n### Managing long conversation history\n\nLong conversations pose\
  \ a challenge to today's LLMs. The full history may not even fit inside an LLM's\
  \ context window, resulting in an irrecoverable error. Even _if_ your LLM technically\
  \ supports the full context length, most LLMs still perform poorly over long contexts.\
  \ They get \"distracted\" by stale or off-topic content, all while suffering from\
  \ slower response times and higher costs.\n\nManaging short-term memory is an exercise\
  \ of balancing [precision & recall](https://en.wikipedia.org/wiki/Precision_and_recall#:~:text=Precision%20can%20be%20seen%20as,irrelevant%20ones%20are%20also%20returned)\
  \ with your application's other performance requirements (latency & cost). As always,\
  \ it's important to think critically about how you represent information for your\
  \ LLM and to look at your data. We cover a few common techniques for managing message\
  \ lists below and hope to provide sufficient context for you to pick the best tradeoffs\
  \ for your application:\n\n- [Editing message lists](#editing-message-lists): How\
  \ to think about trimming and filtering a list of messages before passing to language\
  \ model.\n- [Summarizing past conversations](#summarizing-past-conversations): A\
  \ common technique to use when you don't just want to filter the list of messages.\n\
  \n### Editing message lists\n\nChat models accept context using [messages](https://python.langchain.com/docs/concepts/#messages),\
  \ which include developer provided instructions (a system message) and user inputs\
  \ (human messages). In chat applications, messages alternate between human inputs\
  \ and model responses, resulting in a list of messages that grows longer over time.\
  \ Because context windows are limited and token-rich message lists can be costly,\
  \ many applications can benefit from using techniques to manually remove or forget\
  \ stale information.\n\n![](img/memory/filter.png)\n\nThe most direct approach is\
  \ to remove old messages from a list (similar to a [least-recently used cache](https://en.wikipedia.org/wiki/Page_replacement_algorithm#Least_recently_used)).\n\
  \nThe typical technique for deleting content from a list in LangGraph is to return\
  \ an update from a node telling the system to delete some portion of the list. You\
  \ get to define what this update looks like, but a common approach would be to let\
  \ you return an object or dictionary specifying which values to retain.\n\n```python\n\
  def manage_list(existing: list, updates: Union[list, dict]):\n    if isinstance(updates,\
  \ list):\n        # Normal case, add to the history\n        return existing + updates\n\
  \    elif isinstance(updates, dict) and updates[\"type\"] == \"keep\":\n       \
  \ # You get to decide what this looks like.\n        # For example, you could simplify\
  \ and just accept a string \"DELETE\"\n        # and clear the entire list.\n  \
  \      return existing[updates[\"from\"]:updates[\"to\"]]\n    # etc. We define\
  \ how to interpret updates\n\nclass State(TypedDict):\n    my_list: Annotated[list,\
  \ manage_list]\n\ndef my_node(state: State):\n    return {\n        # We return\
  \ an update for the field \"my_list\" saying to\n        # keep only values from\
  \ index -5 to the end (deleting the rest)\n        \"my_list\": {\"type\": \"keep\"\
  , \"from\": -5, \"to\": None}\n    }\n```\n\nLangGraph will call the `manage_list`\
  \ \"[reducer](low_level.md#reducers)\" function any time an update is returned under\
  \ the key \"my_list\". Within that function, we define what types of updates to\
  \ accept. Typically, messages will be added to the existing list (the conversation\
  \ will grow); however, we've also added support to accept a dictionary that lets\
  \ you \"keep\" certain parts of the state. This lets you programmatically drop old\
  \ message context.\n\nAnother common approach is to let you return a list of \"\
  remove\" objects that specify the IDs of all messages to delete. If you're using\
  \ the LangChain messages and the [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)\
  \ reducer (or `MessagesState`, which uses the same underlying functionality) in\
  \ LangGraph, you can do this using a `RemoveMessage`.\n\n```python\nfrom langchain_core.messages\
  \ import RemoveMessage, AIMessage\nfrom langgraph.graph import add_messages\n# ...\
  \ other imports\n\nclass State(TypedDict):\n    # add_messages will default to upserting\
  \ messages by ID to the existing list\n    # if a RemoveMessage is returned, it\
  \ will delete the message in the list by ID\n    messages: Annotated[list, add_messages]\n\
  \ndef my_node_1(state: State):\n    # Add an AI message to the `messages` list in\
  \ the state\n    return {\"messages\": [AIMessage(content=\"Hi\")]}\n\ndef my_node_2(state:\
  \ State):\n    # Delete all but the last 2 messages from the `messages` list in\
  \ the state\n    delete_messages = [RemoveMessage(id=m.id) for m in state['messages'][:-2]]\n\
  \    return {\"messages\": delete_messages}\n\n```\n\nIn the example above, the\
  \ `add_messages` reducer allows us to [append](https://langchain-ai.github.io/langgraph/concepts/low_level/#serialization)\
  \ new messages to the `messages` state key as shown in `my_node_1`. When it sees\
  \ a `RemoveMessage`, it will delete the message with that ID from the list (and\
  \ the RemoveMessage will then be discarded). For more information on LangChain-specific\
  \ message handling, check out [this how-to on using `RemoveMessage` ](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/).\n\
  \nSee this how-to [guide](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)\
  \ and module 2 from our [LangChain Academy](https://github.com/langchain-ai/langchain-academy/tree/main/module-2)\
  \ course for example usage.\n\n### Summarizing past conversations\n\nThe problem\
  \ with trimming or removing messages, as shown above, is that we may lose information\
  \ from culling of the message queue. Because of this, some applications benefit\
  \ from a more sophisticated approach of summarizing the message history using a\
  \ chat model.\n\n![](img/memory/summary.png)\n\nSimple prompting and orchestration\
  \ logic can be used to achieve this. As an example, in LangGraph we can extend the\
  \ [MessagesState](https://langchain-ai.github.io/langgraph/concepts/low_level/#working-with-messages-in-graph-state)\
  \ to include a `summary` key.\n\n```python\nfrom langgraph.graph import MessagesState\n\
  class State(MessagesState):\n    summary: str\n```\n\nThen, we can generate a summary\
  \ of the chat history, using any existing summary as context for the next summary.\
  \ This `summarize_conversation` node can be called after some number of messages\
  \ have accumulated in the `messages` state key.\n\n```python\ndef summarize_conversation(state:\
  \ State):\n\n    # First, we get any existing summary\n    summary = state.get(\"\
  summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n \
  \       # A summary already exists\n        summary_message = (\n            f\"\
  This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"\
  Extend the summary by taking into account the new messages above:\"\n        )\n\
  \n    else:\n        summary_message = \"Create a summary of the conversation above:\"\
  \n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n\
  \    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent\
  \ messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"\
  ][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n\
  ```\n\nSee this how-to [here](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)\
  \ and module 2 from our [LangChain Academy](https://github.com/langchain-ai/langchain-academy/tree/main/module-2)\
  \ course for example usage.\n\n### Knowing **when** to remove messages\n\nMost LLMs\
  \ have a maximum supported context window (denominated in tokens). A simple way\
  \ to decide when to truncate messages is to count the tokens in the message history\
  \ and truncate whenever it approaches that limit. Naive truncation is straightforward\
  \ to implement on your own, though there are a few \"gotchas\". Some model APIs\
  \ further restrict the sequence of message types (must start with human message,\
  \ cannot have consecutive messages of the same type, etc.). If you're using LangChain,\
  \ you can use the [`trim_messages`](https://python.langchain.com/docs/how_to/trim_messages/#trimming-based-on-token-count)\
  \ utility and specify the number of tokens to keep from the list, as well as the\
  \ `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.\n\
  \nBelow is an example.\n\n```python\nfrom langchain_core.messages import trim_messages\n\
  trim_messages(\n    messages,\n    # Keep the last <= n_count tokens of the messages.\n\
  \    strategy=\"last\",\n    # Remember to adjust based on your model\n    # or\
  \ else pass a custom token_encoder\n    token_counter=ChatOpenAI(model=\"gpt-4\"\
  ),\n    # Remember to adjust based on the desired conversation\n    # length\n \
  \   max_tokens=45,\n    # Most chat models expect that chat history starts with\
  \ either:\n    # (1) a HumanMessage or\n    # (2) a SystemMessage followed by a\
  \ HumanMessage\n    start_on=\"human\",\n    # Most chat models expect that chat\
  \ history ends with either:\n    # (1) a HumanMessage or\n    # (2) a ToolMessage\n\
  \    end_on=(\"human\", \"tool\"),\n    # Usually, we want to keep the SystemMessage\n\
  \    # if it's present in the original history.\n    # The SystemMessage has special\
  \ instructions for the model.\n    include_system=True,\n)\n```\n\n## Long-term\
  \ memory\n\nLong-term memory in LangGraph allows systems to retain information across\
  \ different conversations or sessions. Unlike short-term memory, which is **thread-scoped**,\
  \ long-term memory is saved within custom \"namespaces.\"\n\n### Storing memories\n\
  \nLangGraph stores long-term memories as JSON documents in a [store](persistence.md#memory-store)\
  \ ([reference doc](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)).\
  \ Each memory is organized under a custom `namespace` (similar to a folder) and\
  \ a distinct `key` (like a filename). Namespaces often include user or org IDs or\
  \ other labels that makes it easier to organize information. This structure enables\
  \ hierarchical organization of memories. Cross-namespace searching is then supported\
  \ through content filters. See the example below for an example.\n\n```python\n\
  from langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str])\
  \ -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain\
  \ embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves\
  \ data to an in-memory dictionary. Use a DB-backed store in production use.\nstore\
  \ = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\nuser_id = \"my-user\"\n\
  application_context = \"chitchat\"\nnamespace = (user_id, application_context)\n\
  store.put(\n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n   \
  \         \"User likes short, direct language\",\n            \"User only speaks\
  \ English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n\
  # get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\")\n# search\
  \ for \"memories\" within this namespace, filtering on content equivalence, sorted\
  \ by vector similarity\nitems = store.search(\n    namespace, filter={\"my-key\"\
  : \"my-value\"}, query=\"language preferences\"\n)\n```\n\n### Framework for thinking\
  \ about long-term memory\n\nLong-term memory is a complex challenge without a one-size-fits-all\
  \ solution. However, the following questions provide a structure framework to help\
  \ you navigate the different techniques:\n\n**What is the type of memory?**\n\n\
  Humans use memories to remember [facts](https://en.wikipedia.org/wiki/Semantic_memory),\
  \ [experiences](https://en.wikipedia.org/wiki/Episodic_memory), and [rules](https://en.wikipedia.org/wiki/Procedural_memory).\
  \ AI agents can use memory in the same ways. For example, AI agents can use memory\
  \ to remember specific facts about a user to accomplish a task. We expand on several\
  \ types of memories in the [section below](#memory-types).\n\n**When do you want\
  \ to update memories?**\n\nMemory can be updated as part of an agent's application\
  \ logic (e.g. \"on the hot path\"). In this case, the agent typically decides to\
  \ remember facts before responding to a user. Alternatively, memory can be updated\
  \ as a background task (logic that runs in the background / asynchronously and generates\
  \ memories). We explain the tradeoffs between these approaches in the [section below](#writing-memories).\n\
  \n## Memory types\n\nDifferent applications require various types of memory. Although\
  \ the analogy isn't perfect, examining [human memory types](https://www.psychologytoday.com/us/basics/memory/types-of-memory?ref=blog.langchain.dev)\
  \ can be insightful. Some research (e.g., the [CoALA paper](https://arxiv.org/pdf/2309.02427))\
  \ have even mapped these human memory types to those used in AI agents.\n\n| Memory\
  \ Type | What is Stored | Human Example | Agent Example |\n|-------------|----------------|---------------|---------------|\n\
  | Semantic | Facts | Things I learned in school | Facts about a user |\n| Episodic\
  \ | Experiences | Things I did | Past agent actions |\n| Procedural | Instructions\
  \ | Instincts or motor skills | Agent system prompt |\n\n### Semantic Memory\n\n\
  [Semantic memory](https://en.wikipedia.org/wiki/Semantic_memory), both in humans\
  \ and AI agents, involves the retention of specific facts and concepts. In humans,\
  \ it can include information learned in school and the understanding of concepts\
  \ and their relationships. For AI agents, semantic memory is often used to personalize\
  \ applications by remembering facts or concepts from past interactions. \n\n> Note:\
  \ Not to be confused with \"semantic search\" which is a technique for finding similar\
  \ content using \"meaning\" (usually as embeddings). Semantic memory is a term from\
  \ psychology, referring to storing facts and knowledge, while semantic search is\
  \ a method for retrieving information based on meaning rather than exact matches.\n\
  \n\n#### Profile\n\nSemantic memories can be managed in different ways. For example,\
  \ memories can be a single, continuously updated \"profile\" of well-scoped and\
  \ specific information about a user, organization, or other entity (including the\
  \ agent itself). A profile is generally just a JSON document with various key-value\
  \ pairs you've selected to represent your domain. \n\nWhen remembering a profile,\
  \ you will want to make sure that you are **updating** the profile each time. As\
  \ a result, you will want to pass in the previous profile and [ask the model to\
  \ generate a new profile](https://github.com/langchain-ai/memory-template) (or some\
  \ [JSON patch](https://github.com/hinthornw/trustcall) to apply to the old profile).\
  \ This can be become error-prone as the profile gets larger, and may benefit from\
  \ splitting a profile into multiple documents or **strict** decoding when generating\
  \ documents to ensure the memory schemas remains valid.\n\n![](img/memory/update-profile.png)\n\
  \n#### Collection\n\nAlternatively, memories can be a collection of documents that\
  \ are continuously updated and extended over time. Each individual memory can be\
  \ more narrowly scoped and easier to generate, which means that you're less likely\
  \ to **lose** information over time. It's easier for an LLM to generate _new_ objects\
  \ for new information than reconcile new information with an existing profile. As\
  \ a result, a document collection tends to lead to [higher recall downstream](https://en.wikipedia.org/wiki/Precision_and_recall).\n\
  \nHowever, this shifts some complexity memory updating. The model must now _delete_\
  \ or _update_ existing items in the list, which can be tricky. In addition, some\
  \ models may default to over-inserting and others may default to over-updating.\
  \ See the [Trustcall](https://github.com/hinthornw/trustcall) package for one way\
  \ to manage this and consider evaluation (e.g., with a tool like [LangSmith](https://docs.smith.langchain.com/tutorials/Developers/evaluation))\
  \ to help you tune the behavior.\n\nWorking with document collections also shifts\
  \ complexity to memory **search** over the list. The `Store` currently supports\
  \ both [semantic search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.query)\
  \ and [filtering by content](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.filter).\n\
  \nFinally, using a collection of memories can make it challenging to provide comprehensive\
  \ context to the model. While individual memories may follow a specific schema,\
  \ this structure might not capture the full context or relationships between memories.\
  \ As a result, when using these memories to generate responses, the model may lack\
  \ important contextual information that would be more readily available in a unified\
  \ profile approach.\n\n![](img/memory/update-list.png)\n\nRegardless of memory management\
  \ approach, the central point is that the agent will use the semantic memories to\
  \ [ground its responses](https://python.langchain.com/docs/concepts/rag/), which\
  \ often leads to more personalized and relevant interactions.\n\n### Episodic Memory\n\
  \n[Episodic memory](https://en.wikipedia.org/wiki/Episodic_memory), in both humans\
  \ and AI agents, involves recalling past events or actions. The [CoALA paper](https://arxiv.org/pdf/2309.02427)\
  \ frames this well: facts can be written to semantic memory, whereas *experiences*\
  \ can be written to episodic memory. For AI agents, episodic memory is often used\
  \ to help an agent remember how to accomplish a task. \n\nIn practice, episodic\
  \ memories are often implemented through [few-shot example prompting](https://python.langchain.com/docs/concepts/few_shot_prompting/),\
  \ where agents learn from past sequences to perform tasks correctly. Sometimes it's\
  \ easier to \"show\" than \"tell\" and LLMs learn well from examples. Few-shot learning\
  \ lets you [\"program\"](https://x.com/karpathy/status/1627366413840322562) your\
  \ LLM by updating the prompt with input-output examples to illustrate the intended\
  \ behavior. While various [best-practices](https://python.langchain.com/docs/concepts/#1-generating-examples)\
  \ can be used to generate few-shot examples, often the challenge lies in selecting\
  \ the most relevant examples based on user input.\n\nNote that the memory [store](persistence.md#memory-store)\
  \ is just one way to store data as few-shot examples. If you want to have more developer\
  \ involvement, or tie few-shots more closely to your evaluation harness, you can\
  \ also use a [LangSmith Dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection)\
  \ to store your data. Then dynamic few-shot example selectors can be used out-of-the\
  \ box to achieve this same goal. LangSmith will index the dataset for you and enable\
  \ retrieval of few shot examples that are most relevant to the user input based\
  \ upon keyword similarity ([using a BM25-like algorithm](https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection)\
  \ for keyword based similarity). \n\nSee this how-to [video](https://www.youtube.com/watch?v=37VaU7e7t5o)\
  \ for example usage of dynamic few-shot example selection in LangSmith. Also, see\
  \ this [blog post](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/)\
  \ showcasing few-shot prompting to improve tool calling performance and this [blog\
  \ post](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/)\
  \ using few-shot example to align an LLMs to human preferences.\n\n### Procedural\
  \ Memory\n\n[Procedural memory](https://en.wikipedia.org/wiki/Procedural_memory),\
  \ in both humans and AI agents, involves remembering the rules used to perform tasks.\
  \ In humans, procedural memory is like the internalized knowledge of how to perform\
  \ tasks, such as riding a bike via basic motor skills and balance. Episodic memory,\
  \ on the other hand, involves recalling specific experiences, such as the first\
  \ time you successfully rode a bike without training wheels or a memorable bike\
  \ ride through a scenic route. For AI agents, procedural memory is a combination\
  \ of model weights, agent code, and agent's prompt that collectively determine the\
  \ agent's functionality. \n\nIn practice, it is fairly uncommon for agents to modify\
  \ their model weights or rewrite their code. However, it is more common for agents\
  \ to [modify their own prompts](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator).\
  \ \n\nOne effective approach to refining an agent's instructions is through [\"\
  Reflection\"](https://blog.langchain.dev/reflection-agents/) or meta-prompting.\
  \ This involves prompting the agent with its current instructions (e.g., the system\
  \ prompt) along with recent conversations or explicit user feedback. The agent then\
  \ refines its own instructions based on this input. This method is particularly\
  \ useful for tasks where instructions are challenging to specify upfront, as it\
  \ allows the agent to learn and adapt from its interactions.\n\nFor example, we\
  \ built a [Tweet generator](https://www.youtube.com/watch?v=Vn8A3BxfplE) using external\
  \ feedback and prompt re-writing to produce high-quality paper summaries for Twitter.\
  \ In this case, the specific summarization prompt was difficult to specify *a priori*,\
  \ but it was fairly easy for a user to critique the generated Tweets and provide\
  \ feedback on how to improve the summarization process. \n\nThe below pseudo-code\
  \ shows how you might implement this with the LangGraph memory [store](persistence.md#memory-store),\
  \ using the store to save a prompt, the `update_instructions` node to get the current\
  \ prompt (as well as feedback from the conversation with the user captured in `state[\"\
  messages\"]`), update the prompt, and save the new prompt back to the store. Then,\
  \ the `call_model` get the updated prompt from the store and uses it to generate\
  \ a response.\n\n```python\n# Node that *uses* the instructions\ndef call_model(state:\
  \ State, store: BaseStore):\n    namespace = (\"agent_instructions\", )\n    instructions\
  \ = store.get(namespace, key=\"agent_a\")[0]\n    # Application logic\n    prompt\
  \ = prompt_template.format(instructions=instructions.value[\"instructions\"])\n\
  \    ...\n\n# Node that updates instructions\ndef update_instructions(state: State,\
  \ store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions\
  \ = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"\
  instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n\
  \    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\"\
  ,), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n```\n\n![](img/memory/update-instructions.png)\n\
  \n## Writing memories\n\nWhile [humans often form long-term memories during sleep](https://medicine.yale.edu/news-article/sleeps-crucial-role-in-preserving-memory/),\
  \ AI agents need a different approach. When and how should agents create new memories?\
  \ There are at least two primary methods for agents to write memories: \"on the\
  \ hot path\" and \"in the background\".\n\n![](img/memory/hot_path_vs_background.png)\n\
  \n### Writing memories in the hot path\n\nCreating memories during runtime offers\
  \ both advantages and challenges. On the positive side, this approach allows for\
  \ real-time updates, making new memories immediately available for use in subsequent\
  \ interactions. It also enables transparency, as users can be notified when memories\
  \ are created and stored.\n\nHowever, this method also presents challenges. It may\
  \ increase complexity if the agent requires a new tool to decide what to commit\
  \ to memory. In addition, the process of reasoning about what to save to memory\
  \ can impact agent latency. Finally, the agent must multitask between memory creation\
  \ and its other responsibilities, potentially affecting the quantity and quality\
  \ of memories created.\n\nAs an example, ChatGPT uses a [save_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/)\
  \ tool to upsert memories as content strings, deciding whether and how to use this\
  \ tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent)\
  \ template as an reference implementation.\n\n### Writing memories in the background\n\
  \nCreating memories as a separate background task offers several advantages. It\
  \ eliminates latency in the primary application, separates application logic from\
  \ memory management, and allows for more focused task completion by the agent. This\
  \ approach also provides flexibility in timing memory creation to avoid redundant\
  \ work.\n\nHowever, this method has its own challenges. Determining the frequency\
  \ of memory writing becomes crucial, as infrequent updates may leave other threads\
  \ without new context. Deciding when to trigger memory formation is also important.\
  \ Common strategies include scheduling after a set time period (with rescheduling\
  \ if new events occur), using a cron schedule, or allowing manual triggers by users\
  \ or the application logic.\n\nSee our [memory-service](https://github.com/langchain-ai/memory-template)\
  \ template as an reference implementation."
